import { collectLinks, type CollectionOptions, type CollectionResult } from 'web-link-collector';
import { writeFileSync } from 'fs';
import { FILTER_PRESETS, type DetailedCrawlResult, type CrawlError } from './types.js';

// é«˜åº¦ãªå‹å®‰å…¨ãªã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹
class AdvancedWebCrawler {
  private readonly baseOptions: Partial<CollectionOptions>;
  private readonly errors: CrawlError[] = [];

  constructor(baseOptions: Partial<CollectionOptions> = {}) {
    this.baseOptions = {
      delayMs: 1000,
      selector: 'a',
      ...baseOptions,
    };
  }

  // è¤‡æ•°URLã®ä¸¦åˆ—ã‚¯ãƒ­ãƒ¼ãƒ«
  async crawlMultipleUrls(
    urls: string[],
    options: Partial<CollectionOptions> = {}
  ): Promise<DetailedCrawlResult[]> {
    const mergedOptions = { ...this.baseOptions, ...options };
    
    const promises = urls.map(url => 
      this.crawlSingleUrl(url, mergedOptions)
    );

    return Promise.all(promises);
  }

  // å˜ä¸€URLã®ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
  private async crawlSingleUrl(
    url: string,
    options: Partial<CollectionOptions>
  ): Promise<DetailedCrawlResult> {
    try {
      const startTime = Date.now();
      const result = await collectLinks(url, {
        depth: 1,
        ...options,
      } as CollectionOptions);

      const endTime = Date.now();
      const statistics = this.calculateDetailedStatistics(result, endTime - startTime);

      return {
        ...result,
        statistics,
      };
    } catch (error) {
      const crawlError: CrawlError = {
        url,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        depth: 0,
      };

      this.errors.push(crawlError);

      return {
        initialUrl: url,
        depth: 0,
        allCollectedUrls: [],
        linkRelationships: [],
        errors: [{ url, errorType: 'CRAWL_ERROR', message: crawlError.error }],
        stats: {
          startTime: new Date().toISOString(),
          endTime: new Date().toISOString(),
          durationMs: 0,
          totalUrlsScanned: 0,
          totalUrlsCollected: 0,
          maxDepthReached: 0,
        },
        statistics: {
          totalLinks: 0,
          uniqueDomains: 0,
          pagesVisited: 0,
          crawlTime: 0,
          linksByDepth: {},
          errorCount: 1,
        },
      };
    }
  }

  // è©³ç´°çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
  private calculateDetailedStatistics(
    result: CollectionResult,
    actualCrawlTime: number
  ): DetailedCrawlResult['statistics'] {
    const uniqueDomains = new Set(
      result.allCollectedUrls.map(url => {
        try {
          return new globalThis.URL(url).hostname;
        } catch {
          return 'unknown';
        }
      })
    ).size;

    // ãƒªãƒ³ã‚¯é–¢ä¿‚ã‹ã‚‰æ·±åº¦ã‚’æ¨å®š
    const linksByDepth: Record<number, number> = {};
    result.linkRelationships.forEach((rel, index) => {
      const depth = index < 10 ? 1 : 2; // ç°¡å˜ãªæ·±åº¦æ¨å®š
      linksByDepth[depth] = (linksByDepth[depth] || 0) + 1;
    });

    return {
      totalLinks: result.allCollectedUrls.length,
      uniqueDomains,
      pagesVisited: result.stats.totalUrlsScanned,
      crawlTime: actualCrawlTime,
      linksByDepth,
      errorCount: result.errors.length,
    };
  }

  // JSONå½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
  exportToJson(results: DetailedCrawlResult[], filename: string): void {
    const exportData = {
      timestamp: new Date().toISOString(),
      crawlCount: results.length,
      totalLinks: results.reduce((sum, result) => sum + result.statistics.totalLinks, 0),
      totalErrors: results.reduce((sum, result) => sum + result.statistics.errorCount, 0),
      results,
      errors: this.errors,
    };

    writeFileSync(filename, JSON.stringify(exportData, null, 2));
    console.log(`ğŸ’¾ Results exported to ${filename}`);
  }

  // ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
  getErrorReport(): string {
    if (this.errors.length === 0) {
      return 'âœ… No errors occurred during crawling.';
    }

    let report = `âŒ ${this.errors.length} errors occurred:\n`;
    this.errors.forEach((error, index) => {
      report += `  ${index + 1}. ${error.url}: ${error.error}\n`;
    });

    return report;
  }
}

// é«˜åº¦ãªä½¿ç”¨ä¾‹
async function advancedExample(): Promise<void> {
  console.log('ğŸ”¥ Advanced TypeScript WebLinkCollector Example');
  console.log('===============================================\n');

  const crawler = new AdvancedWebCrawler({
    depth: 2,
    delayMs: 1500,
  });

  try {
    // è¤‡æ•°ã®ã‚µã‚¤ãƒˆã‚’ä¸¦åˆ—ã§ã‚¯ãƒ­ãƒ¼ãƒ«
    const urls = [
      'https://developer.mozilla.org',
      'https://docs.github.com',
      'https://nodejs.org',
    ];

    console.log('ğŸ•¸ï¸  Starting parallel crawl of multiple sites...');
    const results = await crawler.crawlMultipleUrls(urls, {
      filters: FILTER_PRESETS.documentation,
      depth: 1,
    });

    console.log('âœ… Crawl completed!\n');

    // çµæœã®è©³ç´°åˆ†æ
    results.forEach((result, index) => {
      const url = urls[index];
      console.log(`ğŸ“Š Results for ${url}:`);
      console.log(`   - Links found: ${result.statistics.totalLinks}`);
      console.log(`   - Unique domains: ${result.statistics.uniqueDomains}`);
      console.log(`   - Pages visited: ${result.statistics.pagesVisited}`);
      console.log(`   - Crawl time: ${result.statistics.crawlTime}ms`);
      console.log(`   - Errors: ${result.statistics.errorCount}`);
      console.log('');
    });

    // é›†ç´„çµ±è¨ˆ
    const totalLinks = results.reduce((sum, result) => sum + result.statistics.totalLinks, 0);
    const totalTime = results.reduce((sum, result) => sum + result.statistics.crawlTime, 0);
    const totalPages = results.reduce((sum, result) => sum + result.statistics.pagesVisited, 0);

    console.log('ğŸ“ˆ Aggregate Statistics:');
    console.log(`   - Total links: ${totalLinks}`);
    console.log(`   - Total time: ${totalTime}ms`);
    console.log(`   - Total pages visited: ${totalPages}`);
    console.log(`   - Average links per page: ${totalPages > 0 ? Math.round(totalLinks / totalPages) : 0}`);
    console.log('');

    // çµæœã‚’JSONã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    crawler.exportToJson(results, 'advanced-crawl-results.json');

    // ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    console.log(crawler.getErrorReport());

  } catch (error) {
    console.error('âŒ Fatal error:', error);
    if (error instanceof Error) {
      console.error('Stack trace:', error.stack);
    }
  }
}

// å‹å®‰å…¨ãªè¨­å®šãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä½¿ç”¨ä¾‹ï¼‰
 
function _validateCrawlOptions(url: string, options: CollectionOptions): boolean {
  const errors: string[] = [];

  if (!url || typeof url !== 'string') {
    errors.push('URL is required and must be a string');
  }

  if (options.depth !== undefined && (options.depth < 1 || options.depth > 5)) {
    errors.push('Depth must be between 1 and 5');
  }

  if (options.delayMs !== undefined && options.delayMs < 0) {
    errors.push('Delay cannot be negative');
  }

  if (errors.length > 0) {
    console.error('âŒ Configuration errors:');
    errors.forEach(error => console.error(`   - ${error}`));
    return false;
  }

  return true;
}

// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
async function main(): Promise<void> {
  try {
    await advancedExample();
  } catch (error) {
    console.error('Fatal error:', error);
    process.exit(1);
  }
}

// å®Ÿè¡Œ
main().catch(console.error);